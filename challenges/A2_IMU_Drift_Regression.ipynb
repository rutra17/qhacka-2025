{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ab8ce0",
   "metadata": {},
   "source": [
    "# A2 — Regressão de Deriva da IMU (Fácil)\n",
    "\n",
    "Estime a magnitude do bias (deriva) do giroscópio a partir de janelas curtas de IMU. Prefira **feature map quântico + ridge** ou **regressor VQC**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cb859",
   "metadata": {},
   "source": [
    "### Contexto\n",
    "Uma IMU (Unidade de Medição Inercial) fornece sensoriamento de movimento em 6 eixos: acelerômetro 3 eixos $(a_x,a_y,a_z)$ e giroscópio 3 eixos $(g_x,g_y,g_z)$. Giroscópios medem velocidade angular, mas sensores reais sofrem com um offset lentamente variável (bias) que acumula erro quando integrado.\n",
    "\n",
    "#### O que é bias/deriva do giroscópio?\n",
    "Modelo simples: $g_{\\text{meas}}(t) = g_{\\text{true}}(t) + b_g + \\eta(t)$, onde $b_g$ é um bias quase constante e $\\eta$ é ruído. Mesmo um bias pequeno (ex.: $0.01\\text{–}0.03$ rad/s) causa deriva perceptível de rumo ao longo de minutos.\n",
    "\n",
    "#### Por que estimar o bias?\n",
    "- Estabilizar estimativas de atitude/rumo em odometria por rodas/pernas e VIO.\n",
    "- Melhorar estimação de estado em períodos de baixa excitação (parado/pairando).\n",
    "- Reduzir frequência de recalibração e permitir compensação online em controladores.\n",
    "\n",
    "#### Dataset e alvo\n",
    "Cada amostra é uma janela curta $(W\\!\\times\\!6)$ da IMU. O alvo é a magnitude escalar do bias do giroscópio $\\lVert b_g \\rVert$ sintetizada por [`common.data_utils.imu_drift_dataset`](common/data_utils.py). O comprimento da janela ($W=32$) mantém custo computacional e contagem de qubits gerenciáveis.\n",
    "\n",
    "#### Por que métodos quânticos aqui?\n",
    "Após comprimir janelas em estatísticas robustas, a regressão pode se beneficiar de feature maps não lineares expressivos. Feature maps quânticos ou pequenos VQCs atuam como enriquecedores de kernel sob orçamento reduzido de qubits, potencialmente melhorando generalização em regime de poucos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935dff1",
   "metadata": {},
   "source": [
    "## Especificação de Entrada e Saída\n",
    "**Entrada:** `X_train (N×W×6)`, `y_train (N,)`, `X_test (M×W×6)` com $W=32$ nos dados fornecidos  \n",
    "**Saída:** `y_pred (M,)` magnitude do bias em rad/s  \n",
    "**Tipo retornado:** array float (ex.: `np.float64`)\n",
    "\n",
    "Faixa esperada real do bias ≈ $0.01$–$0.03$ rad/s; previsões fora de $[0.0, 0.05]$ são possivelmente errôneas.  \n",
    "Orientações de recursos: qubits ≤ 10, passos do otimizador ≤ 150; evitar loops Python pesados por amostra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa99871",
   "metadata": {},
   "source": [
    "### Dicas de engenharia de características\n",
    "- O bias aparece como offset quase constante nos canais do giroscópio (índices 3–5).\n",
    "- A média temporal dos eixos do giroscópio é forte estimador; variância ajuda a separar ruído vs. deriva.\n",
    "- Extraia (média, mediana, desvio padrão, MAD) por eixo do giroscópio; agregue estatísticas do acelerômetro para reduzir correlação com movimento.\n",
    "- Abordagem quântica: condense estatísticas em embedding de baixa dimensão e aplique um feature map quântico antes de regressão linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ecfad",
   "metadata": {},
   "source": [
    "### Notas de pré-processamento\n",
    "- Padronize (zero média, desvio padrão 1) as features antes da regressão para estabilizar a ridge.\n",
    "- Evite vazamento de estatísticas do teste: ajuste scalers só em treino.\n",
    "- Mantenha qubits modestos (≤10) reduzindo para vetor compacto antes da codificação quântica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d2fad",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Adiciona o diretório pai (qhacka-2025) ao caminho do Python\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import numpy as np\n",
    "from common import data_utils as du\n",
    "from common import baselines as bl  \n",
    "from common import quantum_utils as qu \n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a124c",
   "metadata": {},
   "source": [
    "## Baseline (referência)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e42d9",
   "metadata": {},
   "source": [
    "_Orientação de baseline:_ em seeds variados com [`common.data_utils.imu_drift_dataset`](common/data_utils.py) o ridge clássico tipicamente obtém MAE $0.012\\text{–}0.018$ rad/s; o limite público em $0.020$ deixa margem para melhorias quânticas. Mantenha previsões dentro do intervalo plausível $[0.0,0.05]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7932a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 0.0049 rad/s\n"
     ]
    }
   ],
   "source": [
    "Xtr, ytr = du.imu_drift_dataset(n=320, w=32, seed=0)\n",
    "Xte, yte = du.imu_drift_dataset(n=80,  w=32, seed=5)\n",
    "yp = bl.baseline_imu_reg(Xtr, ytr, Xte)\n",
    "mae = np.mean(np.abs(yp - yte))\n",
    "print(f\"Baseline MAE: {mae:.4f} rad/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37486dae",
   "metadata": {},
   "source": [
    "## Sua tarefa: implementar `solve(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac15865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(Xtr, ytr, Xte):\n",
    "    \n",
    "    # --- Início da Solução VQC para Regressão ---\n",
    "\n",
    "    # 1. Importar bibliotecas\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as pnp\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from common import quantum_utils as qu\n",
    "\n",
    "    # 2. Engenharia de Features (Clássica)\n",
    "    # Xtr tem shape (N, 32, 6)\n",
    "    def extract_features(X):\n",
    "        # Média e desvio padrão ao longo do eixo do tempo (axis=1)\n",
    "        mean = X.mean(axis=1)\n",
    "        std = X.std(axis=1)\n",
    "        # Concatena em um vetor de 12 features (6 médias + 6 desvios)\n",
    "        return pnp.concatenate([mean, std], axis=1)\n",
    "\n",
    "    Xtr_feat = extract_features(Xtr)\n",
    "    Xte_feat = extract_features(Xte)\n",
    "\n",
    "    # 3. Pré-processamento (Scaler + PCA)\n",
    "    # 6 qubits para velocidade\n",
    "    n_qubits = 6\n",
    "    \n",
    "    # Normalizar features X\n",
    "    x_scaler = StandardScaler()\n",
    "    Xtr_norm = x_scaler.fit_transform(Xtr_feat)\n",
    "    Xte_norm = x_scaler.transform(Xte_feat)\n",
    "\n",
    "    pca = PCA(n_components=n_qubits)\n",
    "    Xtr_pca = pca.fit_transform(Xtr_norm)\n",
    "    Xte_pca = pca.transform(Xte_norm)\n",
    "\n",
    "    # 4. Normalizar Alvos Y (MUITO IMPORTANTE)\n",
    "    # O VQC retorna valores em [-1, 1]. Nossos alvos 'y' (ytr) estão em [0.01, 0.03].\n",
    "    # Vamos normalizar 'ytr' para que tenha média 0 e desvio 1.\n",
    "    y_scaler = StandardScaler()\n",
    "    # .reshape(-1, 1) é necessário para o scaler\n",
    "    ytr_norm = y_scaler.fit_transform(ytr.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # 5. Configurar o VQC (usando o utilitário do hackathon)\n",
    "    n_layers = 2\n",
    "    # `make_classifier` nos dá um VQC perfeito para regressão\n",
    "    # (ele retorna um único valor de -1 a 1)\n",
    "    circuit, weight_shapes, dev = qu.make_classifier(n_qubits=n_qubits, n_layers=n_layers, shots=None)\n",
    "\n",
    "    # 6. Definir a Função de Custo (Erro Quadrático Médio)\n",
    "    # O primeiro argumento (argnum=0) são os 'weights'\n",
    "    def cost(weights, x_sample, y_target_sample):\n",
    "        # O circuito retorna um valor em [-1, 1]\n",
    "        pred = circuit(x_sample, weights) \n",
    "        # Comparamos com o y normalizado\n",
    "        loss = (pred - y_target_sample)**2 \n",
    "        return loss\n",
    "\n",
    "    # 7. Treinamento\n",
    "    n_steps = 150 #\n",
    "    \n",
    "    weights = pnp.random.random(size=weight_shapes[\"weights\"], requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "    \n",
    "    # Criamos a função de gradiente para o argumento 0 (weights)\n",
    "    grad_fn = qml.grad(cost, argnum=0)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        idx = pnp.random.randint(0, len(Xtr_pca))\n",
    "        x_sample = Xtr_pca[idx]\n",
    "        y_target_sample = ytr_norm[idx]\n",
    "        \n",
    "        # Forçamos o Autograd a não treinar os dados\n",
    "        x_sample = pnp.array(x_sample, requires_grad=False)\n",
    "        y_target_sample = pnp.array(y_target_sample, requires_grad=False)\n",
    "        \n",
    "        grads = grad_fn(weights, x_sample, y_target_sample)\n",
    "        weights = opt.apply_grad(grads, weights)\n",
    "\n",
    "    # 8. Previsão\n",
    "    preds_norm = []\n",
    "    for x_sample in Xte_pca:\n",
    "        pred = circuit(x_sample, weights)\n",
    "        preds_norm.append(pred)\n",
    "        \n",
    "    preds_norm_array = pnp.array(preds_norm).reshape(-1, 1)\n",
    "    \n",
    "    # 9. Desfazer a Normalização\n",
    "    # Convertemos as previsões [-1, 1] de volta para a escala original [0.01, 0.03]\n",
    "    y_pred = y_scaler.inverse_transform(preds_norm_array)\n",
    "    \n",
    "    return y_pred.flatten() # Retorna o array 1D\n",
    "\n",
    "    # --- Fim da Solução VQC ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166029fa",
   "metadata": {},
   "source": [
    "## Testes públicos (rápidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339dea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public MAE: 0.005137499313060666\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "Xtr, ytr = du.imu_drift_dataset(n=320, w=32, seed=10)\n",
    "Xte, yte = du.imu_drift_dataset(n=80,  w=32, seed=11)\n",
    "yp = solve(Xtr, ytr, Xte)\n",
    "mae = np.mean(np.abs(yp - yte))\n",
    "print(\"Public MAE:\", mae)\n",
    "assert yp.shape == yte.shape\n",
    "assert mae <= 0.020, \"MAE público acima do limiar\"\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375e3ba",
   "metadata": {},
   "source": [
    "> Testes adicionais serão executados pelos organizadores com seeds/tamanhos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0e693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Teste de Validação (Robustedez) ---\n",
      "MAE Validação (seed=99): 0.0044919906000840375\n",
      "MAE Público   (seed=10): 0.005137499313060666\n",
      "Diferença: 0.000646\n",
      "VEREDITO: Solução robusta, não houve overfitting.\n",
      "--- Fim do Teste de Validação ---\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA DE VALIDAÇÃO (seed=99) ---\n",
    "print(\"--- Iniciando Teste de Validação (Robustedez) ---\")\n",
    "\n",
    "# Usar um seed diferente (ex: 99) para simular os testes ocultos da banca\n",
    "Xtr_val, ytr_val = du.imu_drift_dataset(n=320, w=32, seed=99)\n",
    "Xte_val, yte_val = du.imu_drift_dataset(n=80,  w=32, seed=100)\n",
    "\n",
    "# Rodar a mesma solução 'solve'\n",
    "yp_val = solve(Xtr_val, ytr_val, Xte_val)\n",
    "mae_val = np.mean(np.abs(yp_val - yte_val))\n",
    "\n",
    "print(\"MAE Validação (seed=99):\", mae_val)\n",
    "\n",
    "# Pegar o MAE do teste público (variável 'mae' da célula anterior)\n",
    "try:\n",
    "    print(\"MAE Público   (seed=10):\", mae) \n",
    "    print(f\"Diferença: {abs(mae - mae_val):.6f}\")\n",
    "    \n",
    "    # Veredito: 1. A validação tem que passar na meta. 2. A diferença tem que ser pequena.\n",
    "    assert mae_val <= 0.020, \"MAE de validação falhou no limiar!\"\n",
    "    assert abs(mae - mae_val) < 0.01, \"Overfitting detectado! A diferença é muito alta.\"\n",
    "    print(\"VEREDITO: Solução robusta, não houve overfitting.\")\n",
    "except NameError:\n",
    "    print(\"Execute a célula de Teste Público primeiro para comparar.\")\n",
    "\n",
    "print(\"--- Fim do Teste de Validação ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
